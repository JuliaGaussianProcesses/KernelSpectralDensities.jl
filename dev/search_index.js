var documenterSearchIndex = {"docs":
[{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"EditURL = \"../../../../examples/2-features/script.jl\"","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelSpectralDensities.jl/blob/main/examples/2-features/script.jl\"","category":"page"},{"location":"examples/2-features/#Random-Fourier-Features","page":"Random Fourier Features","title":"Random Fourier Features","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"One of the reasons to be interested in the spectral density of a kernel is that it allows us to approximate a GP Prior. In this notebook we show the two feature functions implemented in KernelSpectralDensities.jl and how to use them.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Load required packages","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"using KernelSpectralDensities\nusing StatsBase\nusing LinearAlgebra\nusing CairoMakie","category":"page"},{"location":"examples/2-features/#Intro","page":"Random Fourier Features","title":"Intro","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"In general, feature functions allow us to project an input into a higher-dimensional space, which is useful for a variety of tasks. For example, we can use them to approximate a kernel using the \"kernel trick\".","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"A special class of feature functions are \"random Fourier features\", derived from the Fourier transform, which we saw in add link from other example. KernelSpectralDensities implements two types of random Fourier features, ShiftedRFF and DoubleRFF.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"For this example we use the simple squared exponential kernel.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"ker = SqExponentialKernel()\nS = SpectralDensity(ker, 1);","category":"page"},{"location":"examples/2-features/#ShiftedRFF","page":"Random Fourier Features","title":"ShiftedRFF","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"The ShiftedRFF feature function is somewhat more common, and has been used in papers such as Efficiently sampling functions from Gaussian process posteriors. It is defined as","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"    varphi_i(x) = sqrt2  l  cos(2  π  ((w_i^T  x) + b_i))","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"where w_i is sampled from the spectral density S, b_i is uniformly sampled from 0 2π and l is the number of sampled frequencies, which is also the number of features.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"We generate a set of 4 feature functions, which we can evaluate at any point.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"srff = ShiftedRFF(S, 4)\nsrff(1.0)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"4-element Vector{Float64}:\n  0.6548677131831898\n -0.5200719301446635\n  0.6411909621852575\n -0.5099471013041686","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"If we plot them, we see that each feature function is a harmonic with varying frequency and phase.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"x = range(0, 5; length=100)\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"x\", ylabel=\"rff(x)\", title=\"ShiftedRFF\")\nseries!(ax, x, reduce(hcat, srff.(x)); labels=[\"srff $i\" for i in 1:4])\naxislegend(ax; position=:rt)\nf","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/#DoubleRFF","page":"Random Fourier Features","title":"DoubleRFF","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"The DoubleRFF feature function is less common, but is theoretically equivalent to the ShiftedRFF feature function. It is defined as","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"    varphi(x) = sqrt1  l beginpmatrix cos(2 π w x)  sin(2 π w x) endpmatrix","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"where w is sampled from the spectral density S, with a total of l2 sampled frequencies. Here, each function is effectively two feature functions in one, so specifying l will result in l2 samples but an l dimensional feature vector.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"We again generate a set of 4 feature functions.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"drff = DoubleRFF(S, 4)\ndrff(1.0)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"4-element Vector{Float64}:\n  0.6975708020306294\n  0.6617360792561624\n  0.11573666728545728\n -0.24920947295534734","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"We plot these features as well","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"f = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"x\", ylabel=\"rff(x)\", title=\"Double RFF\")\nseries!(ax, x, reduce(hcat, drff.(x)); labels=[\"drff $i\" for i in 1:4])\naxislegend(ax; position=:rt)\nf","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/#Approximating-a-kernel","page":"Random Fourier Features","title":"Approximating a kernel","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"We can use the feature functions to approximate a kernel, using the kernel trick","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"    k(x x) = langle varphi(x) varphi(x) rangle","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Comparing with the Kernel Densities example, we note that this is effective a Monte Carlo approximation of inverse Fourier transform.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"To demonstrate that this works, we generate some feature functions and see how well they recover the kernel.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"rff = ShiftedRFF(S, 100)\nkt(x, y) = dot(rff(x), rff(y))\n\nx_plot = range(0, 2; length=50)\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"y\", ylabel=\"ker(0, y)\", title=\"\")\nlines!(ax, x_plot, ker.(0, x_plot); label=\"Original Kernel\")\nlines!(ax, x_plot, kt.(0, x_plot); label=\"KT, l = 100\")\naxislegend(ax)\nf","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Clearly this is not quite correct, and we can quantify this by checking the error.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"norm(ker.(0, x_plot) .- kt.(0, x_plot))","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.4832636722051011","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Fortunately, we can improve the approximation by using more features,","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"rff1000 = ShiftedRFF(S, 5000)\nkt1000(x, y) = dot(rff1000(x), rff1000(y))\n\nlines!(ax, x_plot, kt1000.(0, x_plot); label=\"KT, l=1000\")\naxislegend(ax)\nf","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"which also reduces the error.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"norm(ker.(0, x_plot) .- kt1000.(0, x_plot))","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.03836805701093269","category":"page"},{"location":"examples/2-features/#Comparing-the-RFFs","page":"Random Fourier Features","title":"Comparing the RFFs","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"In the section above we used the ShiftedRFF feature function, but what about the DoubleRFF? Let's compare the two!. First we define some helper functions.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"function kt_error(ker, rff, S, l, x)\n    rff = rff(S, l)\n    kt(x, y) = dot(rff(x), rff(y))\n    return norm(ker.(0, x) .- kt.(0, x))\nend\n\nfunction mean_kt_error(ker, rff, S, l, x, n)\n    return mean([kt_error(ker, rff, S, l, x) for _ in 1:n])\nend\nnothing # hide","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Now we compute the mean error for both feature functions, using 100 features when recovering the original kernel. To reduce the effect of randomness, we average over 5000 runs.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"srff_err = mean_kt_error(ker, ShiftedRFF, S, 100, x_plot, 5000)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.5326873604442691","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"drff_err = mean_kt_error(ker, DoubleRFF, S, 100, x_plot, 5000)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.38624944511770737","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"We see that the double rff has a lower average error. This continues to hold for higher number of features.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"srff_err = mean_kt_error(ker, ShiftedRFF, S, 1000, x_plot, 5000)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.16634810006753353","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"drff_err = mean_kt_error(ker, DoubleRFF, S, 1000, x_plot, 5000)","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"0.12353823352267429","category":"page"},{"location":"examples/2-features/#Comparison,-continued","page":"Random Fourier Features","title":"Comparison, continued","text":"","category":"section"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"Lastly, we show a loglog plot of the mean error as a function of the number of features. We see that both feature functions have the order of error scaling, but the DualRFF error has a small offset, resulting in a lower error. This is especially impactful for a small number of features.","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"l_plot = [10, 50, 100, 500, 1000]\nsrff_comp = [mean_kt_error(ker, ShiftedRFF, S, l, x_plot, 5000) for l in l_plot]\ndrff_comp = [mean_kt_error(ker, DoubleRFF, S, l, x_plot, 5000) for l in l_plot]\n\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"l\", ylabel=\"mean err\", title=\"\", xscale=log10, yscale=log10)\nlines!(ax, l_plot, srff_comp; label=\"ShiftedRFF\")\nlines!(ax, l_plot, drff_comp; label=\"DoubleRFF\")\naxislegend(ax)\nf","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"(Image: )","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"<hr />\n<h6>Package and system information</h6>\n<details>\n<summary>Package information (click to expand)</summary>\n<pre>\nStatus &#96;~/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl/examples/2-features/Project.toml&#96;\n  &#91;13f3f980&#93; CairoMakie v0.13.1\n  &#91;0b91fe84&#93; DisplayAs v0.1.6\n  &#91;027d52a2&#93; KernelSpectralDensities v0.2.0 &#96;/home/runner/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl#main&#96;\n  &#91;98b081ad&#93; Literate v2.20.1\n  &#91;2913bbd2&#93; StatsBase v0.34.4\n</pre>\nTo reproduce this notebook's package environment, you can\n<a href=\"./Manifest.toml\">\ndownload the full Manifest.toml</a>.\n</details>\n<details>\n<summary>System information (click to expand)</summary>\n<pre>\nJulia Version 1.11.3\nCommit d63adeda50d &#40;2025-01-21 19:42 UTC&#41;\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux &#40;x86_64-linux-gnu&#41;\n  CPU: 4 × AMD EPYC 7763 64-Core Processor\n  WORD_SIZE: 64\n  LLVM: libLLVM-16.0.6 &#40;ORCJIT, znver3&#41;\nThreads: 1 default, 0 interactive, 1 GC &#40;on 4 virtual cores&#41;\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE &#61; eager\n  JULIA_LOAD_PATH &#61; :/home/runner/.julia/packages/JuliaGPsDocs/7M86H/src\n</pre>\n</details>","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"","category":"page"},{"location":"examples/2-features/","page":"Random Fourier Features","title":"Random Fourier Features","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"EditURL = \"../../../../examples/3-priorGP/script.jl\"","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelSpectralDensities.jl/blob/main/examples/3-priorGP/script.jl\"","category":"page"},{"location":"examples/3-priorGP/#Sampling-a-GP-Prior","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"","category":"section"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"(Image: )","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Load required packages","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"using KernelSpectralDensities\nusing AbstractGPs\nusing StatsBase\nusing LinearAlgebra\nusing CairoMakie","category":"page"},{"location":"examples/3-priorGP/#Intro","page":"Sampling a GP Prior","title":"Intro","text":"","category":"section"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"We use the AbstractGPs package to define a stationary GP prior, in other words, a GP that has not been conditioned on data yet.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ker = SqExponentialKernel()\nS = SpectralDensity(ker, 1)\n\ngp = GP(ker)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"AbstractGPs.GP{AbstractGPs.ZeroMean{Float64}, KernelFunctions.SqExponentialKernel{Distances.Euclidean}}(AbstractGPs.ZeroMean{Float64}(), Squared Exponential Kernel (metric = Distances.Euclidean(0.0)))","category":"page"},{"location":"examples/3-priorGP/#Naive-Sampling","page":"Sampling a GP Prior","title":"Naive Sampling","text":"","category":"section"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"If we want to draw a sample from the GP prior, the standard way is to use the Cholesky decomposition of the kernel matrix.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"In this example, we want to sample the GP at the following points","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"x_sample = range(0, 2; length=5)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"0.0:0.5:2.0","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"To sample, we calculate the mean and covariance of the GP at these points. While we use the AbstractGPs interface, in this case the mean is just a zero vector and the covariance is the kernel matrix over the sample points.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"m = mean(gp, x_sample)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Zeros(5)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"K = cov(gp, x_sample)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 1.0       0.882497  0.606531  0.324652  0.135335\n 0.882497  1.0       0.882497  0.606531  0.324652\n 0.606531  0.882497  1.0       0.882497  0.606531\n 0.324652  0.606531  0.882497  1.0       0.882497\n 0.135335  0.324652  0.606531  0.882497  1.0","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"We then compute the Cholesky decomposition of the covariance matrix samples a vector of standard normal random variables and obtain a sample from the GP prior.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"function naive_sample(gp, x_sample)\n    m = mean(gp, x_sample)\n    K = cov(gp, x_sample)\n    Kc = cholesky(K).L\n    ζ = randn(length(x_sample))\n    return m .+ Kc * ζ\nend\n\nys = naive_sample(gp, x_sample)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n -0.497880456538927\n -0.5662818332870188\n -0.8454795005912243\n -1.1149744375484971\n -0.8091910946072824","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"To illustrate we plot a few samples","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"x_plot = range(0, 1; length=10)\nn_samples = 7\nys_plot = [naive_sample(gp, x_plot) for _ in 1:n_samples]\n\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"x\", ylabel=\"y\", title=\"Naive Sampling\")\nseries!(ax, x_plot, reduce(hcat, ys_plot)'; labels=[\"sample $i\" for i in 1:n_samples])\nf","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"(Image: )","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"To evaluate the samples, we define the following function","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"function evaluate_samples(y_sample, m, K)\n    ms = mean(y_sample)\n    merr = norm(m .- ms)\n    cs = cov(y_sample)\n    cerr = norm(K .- cs)\n    print(\"Mean error: $merr, Covariance error: $cerr\\n\")\n    return ms, cs\nend","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"For the small number of samples we have, the results are not very good.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"y_sample = [naive_sample(gp, x_sample) for _ in 1:n_samples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 0.9863109755153893, Covariance error: 2.8096375521957353\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n 0.10981562476406978\n 0.44031083678823524\n 0.6171010984970332\n 0.5265149839818042\n 0.32991581026533917","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n  1.31714    0.941102   0.307375  -0.305176   -0.751424\n  0.941102   0.777541   0.388235  -0.0304563  -0.39374\n  0.307375   0.388235   0.501834   0.608555    0.535152\n -0.305176  -0.0304563  0.608555   1.31275     1.59242\n -0.751424  -0.39374    0.535152   1.59242     2.15999","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"If we sample a lot more functions however, we get closer to the anaytical result","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"n_manysamples = 1000\ny_sample = [naive_sample(gp, x_sample) for _ in 1:n_manysamples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 0.09112083662239238, Covariance error: 0.08574644006897204\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n -0.052207044811031666\n -0.05962241637332068\n -0.0415980364180494\n -0.01214127943625551\n  0.01203293415935746","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 1.00385   0.873163  0.610198  0.340433  0.144414\n 0.873163  0.97661   0.875248  0.621779  0.347945\n 0.610198  0.875248  1.00342   0.904101  0.636395\n 0.340433  0.621779  0.904101  1.02244   0.892318\n 0.144414  0.347945  0.636395  0.892318  0.970916","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"However, there are two issues with this approach:","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"It is quite computationally expensive, since we need to calculate the Cholesky decomposition.\nSampling at a larger number of points can cause conditionint issues, as we show below.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"x_sample_many = range(0, 2; length=20)\ntry\n    naive_sample(gp, x_sample_many)\ncatch err\n    showerror(stderr, err)\nend","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"PosDefException: matrix is not positive definite; Factorization failed.","category":"page"},{"location":"examples/3-priorGP/#RFF-Sampling","page":"Sampling a GP Prior","title":"RFF Sampling","text":"","category":"section"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Random Fourier features are an alternative option to sample the GP prior. Instead of computing the Cholesky decomposition of the kernel matrix, we compute a number of Fourier features and can generate samples from the GP by defining a weighted sum of these features.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"    f(x) = sum_i=1^l w_i varphi_i(x)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"The weights w_i are sampled from a standard normal distribution.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"rff = DoubleRFF(S, 10)\nagps = ApproximateGPSample(rff)\nagps.(x_sample)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n  1.2810295438770924\n  1.1162140294256733\n  0.7770627207927714\n  0.34160330399027394\n -0.07134896426884496","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"We can plot the samples as before","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"n_samples = 7\nys_plot = [ApproximateGPSample(rff).(x_plot) for _ in 1:n_samples]\n\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"x\", ylabel=\"y\", title=\"RFF Sampling\")\nseries!(ax, x_plot, reduce(hcat, ys_plot)'; labels=[\"sample $i\" for i in 1:n_samples])\nf","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"(Image: )","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Unfortunately, the mean and the covariance are worse than with the naive sampling for the same number of samples.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"y_sample = [ApproximateGPSample(rff).(x_sample) for _ in 1:n_samples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 0.4703725051165074, Covariance error: 1.7487573694832248\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n -0.2916599272198823\n -0.2788164310088815\n -0.20907095337003995\n -0.11527850400740348\n -0.038031318592695","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 0.728927  0.606082  0.467075  0.381741  0.395026\n 0.606082  0.562672  0.477294  0.397215  0.365684\n 0.467075  0.477294  0.445296  0.395448  0.358293\n 0.381741  0.397215  0.395448  0.389309  0.39238\n 0.395026  0.365684  0.358293  0.39238   0.468779","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"However, we now have another parameter to tune: The number of features By increasing the number of features, we get close to the result we saw with the naive sampling.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"rff500 = DoubleRFF(S, 500)\ny_sample = [ApproximateGPSample(rff500).(x_sample) for _ in 1:n_samples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 1.535041408116047, Covariance error: 2.118621506946047\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n 0.9863249796239554\n 0.9531885294759388\n 0.629955041273104\n 0.2548571509962441\n 0.11467896510205029","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 0.986024   1.06929   0.789453  0.389075  0.0986448\n 1.06929    1.59471   1.51785   1.12985   0.812884\n 0.789453   1.51785   1.65726   1.42097   1.15974\n 0.389075   1.12985   1.42097   1.38114   1.25007\n 0.0986448  0.812884  1.15974   1.25007   1.34014","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"By increasing the number of GP samples, we can again improve the results in both cases.","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"With 10 feature functions","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"y_sample = [ApproximateGPSample(rff).(x_sample) for _ in 1:n_manysamples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 0.13897612660738592, Covariance error: 1.215132941499786\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n 0.058474311019376266\n 0.06276453528159073\n 0.06537066457917035\n 0.06444674734720866\n 0.05940559567355604","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 1.0424    1.00334   0.876581  0.712909  0.575138\n 1.00334   1.0641    1.01373   0.874083  0.701494\n 0.876581  1.01373   1.05605   0.989184  0.841873\n 0.712909  0.874083  0.989184  1.01591   0.946257\n 0.575138  0.701494  0.841873  0.946257  0.976566","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"With 500 feature functions","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"y_sample = [ApproximateGPSample(rff500).(x_sample) for _ in 1:n_manysamples]\nms, cs = evaluate_samples(y_sample, m, K);","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Mean error: 0.05521427949286809, Covariance error: 0.1275305259448162\n","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ms","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5-element Vector{Float64}:\n  0.0365486212262885\n  0.03869196883232553\n  0.012671806682106273\n -0.006790936547433723\n  0.003009151121538044","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"cs","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"5×5 Matrix{Float64}:\n 0.999612  0.874209  0.573214  0.28624   0.118205\n 0.874209  1.02554   0.892758  0.579912  0.292827\n 0.573214  0.892758  1.02792   0.880059  0.566414\n 0.28624   0.579912  0.880059  1.00106   0.853468\n 0.118205  0.292827  0.566414  0.853468  0.980865","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"Lastly, we note that we no longer have to worry about conditioning issues, and can evaluate a given GP sample at however many points we like","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"ApproximateGPSample(rff).(x_sample_many)","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"20-element Vector{Float64}:\n -1.6138981711044387\n -1.6110727561309952\n -1.5941675481835103\n -1.5626196248640434\n -1.5161398429978141\n -1.4547193827504028\n -1.3786307593985894\n -1.2884232818760128\n -1.184913047146441\n -1.0691676676267423\n -0.9424860331470348\n -0.8063735072324709\n -0.6625130478735763\n -0.5127328236304161\n -0.3589709652927843\n -0.20323815002041867\n -0.04757875779138393\n  0.10596863175635762\n  0.2554106209548927\n  0.39883747548733645","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"<hr />\n<h6>Package and system information</h6>\n<details>\n<summary>Package information (click to expand)</summary>\n<pre>\nStatus &#96;~/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl/examples/3-priorGP/Project.toml&#96;\n  &#91;99985d1d&#93; AbstractGPs v0.5.21\n  &#91;7834405d&#93; AbstractGPsMakie v0.2.8\n⌃ &#91;13f3f980&#93; CairoMakie v0.12.18\n  &#91;0b91fe84&#93; DisplayAs v0.1.6\n  &#91;027d52a2&#93; KernelSpectralDensities v0.2.0 &#96;/home/runner/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl#main&#96;\n  &#91;98b081ad&#93; Literate v2.20.1\n  &#91;2913bbd2&#93; StatsBase v0.34.4\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n</pre>\nTo reproduce this notebook's package environment, you can\n<a href=\"./Manifest.toml\">\ndownload the full Manifest.toml</a>.\n</details>\n<details>\n<summary>System information (click to expand)</summary>\n<pre>\nJulia Version 1.11.3\nCommit d63adeda50d &#40;2025-01-21 19:42 UTC&#41;\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux &#40;x86_64-linux-gnu&#41;\n  CPU: 4 × AMD EPYC 7763 64-Core Processor\n  WORD_SIZE: 64\n  LLVM: libLLVM-16.0.6 &#40;ORCJIT, znver3&#41;\nThreads: 1 default, 0 interactive, 1 GC &#40;on 4 virtual cores&#41;\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE &#61; eager\n  JULIA_LOAD_PATH &#61; :/home/runner/.julia/packages/JuliaGPsDocs/7M86H/src\n</pre>\n</details>","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"","category":"page"},{"location":"examples/3-priorGP/","page":"Sampling a GP Prior","title":"Sampling a GP Prior","text":"This page was generated using Literate.jl.","category":"page"},{"location":"feature_functions/#Feature-functions","page":"Feature Functions","title":"Feature functions","text":"","category":"section"},{"location":"feature_functions/","page":"Feature Functions","title":"Feature Functions","text":"KernelSpectralDensities.AbstractRFF\nKernelSpectralDensities.ShiftedRFF\nKernelSpectralDensities.DoubleRFF\nKernelSpectralDensities.ApproximateGPSample","category":"page"},{"location":"feature_functions/#KernelSpectralDensities.AbstractRFF","page":"Feature Functions","title":"KernelSpectralDensities.AbstractRFF","text":"AbstractRFF\n\nAbstract type defining a random Fourier feature function. \n\n\n\n\n\n","category":"type"},{"location":"feature_functions/#KernelSpectralDensities.ShiftedRFF","page":"Feature Functions","title":"KernelSpectralDensities.ShiftedRFF","text":"ShiftedRFF([rng::AbstractRNG], S::SpectralDensity, l::Int)\n\nRandom Fourier feature function with a random shift, projecting an input x into l dimensionional feature space. \n\nDefinition\n\nThis feature function is defined as \n\n    sqrt2  l  cos(2  π  ((w^T  x) + b))\n\nwhere w sampled from the spectral density S, b is uniformly sampled from [0, 2π] and l is the number of sampled frequencies.\n\nExamples\n\njulia> k = SqExponentialKernel();\n\njulia> S = SpectralDensity(k, 1);\n\njulia> rff = ShiftedRFF(S, 2);\n\njulia> rff(1.);\n\n\n\n\n\n","category":"type"},{"location":"feature_functions/#KernelSpectralDensities.DoubleRFF","page":"Feature Functions","title":"KernelSpectralDensities.DoubleRFF","text":"DoubleRFF([rng::AbstractRNG], S::SpectralDensity, l::Int)\n\nRandom Fourier feature function with cos and sin terms, projecting an input x into l dimensionional feature space.\n\nDefinition\n\nThis feature function is defined as\n\n    sqrt1  l cos(2 π w x)  sin(2 π w x)\n\nwhere w sampled from the spectral density S, with a total of l/2 sampled frequencies. The output will be the result of [cos(...w_1), cos(...w_2), ..., cos(...w_l/2), sin(...w_1), sin(...w_2), ..., sin(...w_l/2)].\n\nExamples\n\njulia> k = SqExponentialKernel();\n\njulia> S = SpectralDensity(k, 1);\n\njulia> rff = DoubleRFF(S, 2);\n\njulia> rff(1.);\n\n\n\n\n\n","category":"type"},{"location":"feature_functions/#KernelSpectralDensities.ApproximateGPSample","page":"Feature Functions","title":"KernelSpectralDensities.ApproximateGPSample","text":"ApproximateGPSample(rff::AbstractRFF)\n\nAn approximate sample from the GP prior defined by the kernel that corresponds to the spectral density S that the RFF rff is based on.\n\nDefinition\n\nUsing the a vector of l random fourier features r(x), we can define the Bayesian linear model \n\n    g_s(x) = w r(x)\n\nwhere w_i ~ N(0, 1), i = 1,...,l.  Each draw of w results in a different function sample from the GP prior.\n\nExamples\n\njulia> k = SqExponentialKernel();\n\njulia> S = SpectralDensity(k, 1);\n\njulia> rff = DoubleRFF(S, 2);\n\njulia> ap = ApproximateGPSample(rff);\n\njulia> ap(1.);\n\n\n\n\n\n","category":"type"},{"location":"densities/#Spectral-Densities","page":"Spectral Densities","title":"Spectral Densities","text":"","category":"section"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"A kernel k(xy) is described as stationary or shift-invariant, if it can be written as k(τ) = k(x-y), which means that it only depends on the difference between x and y but not their absolute values. ","category":"page"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"For a given stationary kernel k(τ), the spectral density is its Fourier transform","category":"page"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"S(omega) = int_-infty^infty k(τ) e^-2 pi omega^T tau dtau","category":"page"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"note: Note\nThe exact form of the Fourier transform may change slightly between fields (see Details and Options).  This package uses the \"signal processing\" form above, as done in this presentation by Markus Heinonen. However, Rahimi & Recht used the \"classical physics\" form, for example.  All options are equivalent, if used consistently.","category":"page"},{"location":"densities/#Shared-interface","page":"Spectral Densities","title":"Shared interface","text":"","category":"section"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"    KernelSpectralDensities.SpectralDensity\n    Base.rand(::KernelSpectralDensities.AbstractSpectralDensity, ::Int64)","category":"page"},{"location":"densities/#KernelSpectralDensities.SpectralDensity","page":"Spectral Densities","title":"KernelSpectralDensities.SpectralDensity","text":"SpectralDensity{K<:Kernel}(k::Kernel, dim::Int)\n\nSpectral density for the kernel K for dim dimensional frequency space. \n\nDefinition\n\nGiven a stationary kernel k(x x) the spectral density is the Fourier transform of k(τ) = k(x-x).  It can be seen as a probablity density function over the frequency domain, and can be evaluated at any frequency w. \n\nExamples\n\njulia> k = SqExponentialKernel();\n\njulia> S = SpectralDensity(k, 1);\n\njulia> S(0.0)\n2.5066282746310007\n\njulia> S = SpectralDensity(k, 2);\n\njulia> S(zeros(2))\n6.2831853071795845\n\n\n\n\n\n","category":"type"},{"location":"densities/#Base.rand-Tuple{KernelSpectralDensities.AbstractSpectralDensity, Int64}","page":"Spectral Densities","title":"Base.rand","text":"rand(S::AbstractSpectralDensity, [n::Int])\n\nGenerate a sample [n samples] from the spectral density S.  For a scalar spectral density over ω in R, computing n samples results in a n dimensional vector.  For a multi-dimensional spectral density over ω in R^d d1, computing n samples results in a d,n matrix. \n\nExamples\n\njulia> k = SqExponentialKernel();\n\njulia> S = SpectralDensity(k, 1);\n\njulia> rand(S, 1);\n\n\n\n\n\n","category":"method"},{"location":"densities/#Supported-Kernel","page":"Spectral Densities","title":"Supported Kernel","text":"","category":"section"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"Squared Exponential\nAny Matern Kernel","category":"page"},{"location":"densities/#Supported-Transformations","page":"Spectral Densities","title":"Supported Transformations","text":"","category":"section"},{"location":"densities/","page":"Spectral Densities","title":"Spectral Densities","text":"with_lengthscale","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = KernelSpectralDensities","category":"page"},{"location":"#KernelSpectralDensities","page":"Home","title":"KernelSpectralDensities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is a lightweight extension to KernelFunctions.jl, providing the spectral densities for a subset of kernels.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"EditURL = \"../../../../examples/1-densities/script.jl\"","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelSpectralDensities.jl/blob/main/examples/1-densities/script.jl\"","category":"page"},{"location":"examples/1-densities/#Kernel-Densities","page":"Kernel Densities","title":"Kernel Densities","text":"","category":"section"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"(Image: )","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"All stationary kernels can have a spectral density, which is the Fourier transform of the function k(tau) = k(x x), where tau = t - t.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"In other words, the spectral density is defined as","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"  S(omega) = int_-infty^infty k(τ) e^-2 pi omega^T tau dtau","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"In this notebook we show how we can recover the kernel from its spectral density.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"Load required packages","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"using KernelSpectralDensities\nusing Distributions\nusing LinearAlgebra\nusing FastGaussQuadrature\nusing OrderedCollections\n\nusing CairoMakie","category":"page"},{"location":"examples/1-densities/#Intro","page":"Kernel Densities","title":"Intro","text":"","category":"section"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"First we define a few kernels, from KernelFunctions.jl, which is re-exported by KernelSpectralDensities.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"kers = OrderedDict(\n    \"Matern3/2\" => Matern32Kernel(),\n    \"Matern3/2 0.8\" => with_lengthscale(Matern32Kernel(), 0.8),\n    \"Matern3/2 1.2\" => with_lengthscale(Matern32Kernel(), 1.2),\n);","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"We plot them here for illustration.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"τ_interval = [0.0, 4.0]\nτv = range(τ_interval...; length=60)\n\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"τ\", ylabel=\"k(τ)\")\nfor (key, ker) in kers\n    lines!(ax, τv, ker.(0, τv); label=key)\nend\naxislegend()\nf","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"(Image: )","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"Now we can use a function from KernelSpectralDensities.jl to get its spectral density. The resulting object allows us to evaluate the spectral density for any frequency.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"S = SpectralDensity(kers[\"Matern3/2\"], 1)\nS(0.5)","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"0.12549068176931283","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"We can also plot it over the interval we defined to see its shape.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"w_plot = range(-1, 1; length=151)\n\nf = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"ω\", ylabel=\"S(ω)\")\nfor (key, ker) in kers\n    Sp = SpectralDensity(ker, 1)\n    lines!(ax, w_plot, Sp.(w_plot); label=key)\nend\naxislegend()\nf","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"(Image: )","category":"page"},{"location":"examples/1-densities/#Recovering-the-kernel","page":"Kernel Densities","title":"Recovering the kernel","text":"","category":"section"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"We can recover the kernel by integrating the spectral density over all frequencies.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"First, we we define the stationary function and some interals","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"ker = kers[\"Matern3/2\"]\nk(t) = ker(0, t);","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"For the numerical integration we use the GaussLegendre quadrature schema, which is more accurate and efficient than equidistant intervals. This allows us to define a new function, which numerically approximates the inverse Fourier transform of the spectral density.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"w_interval = [-2.0, 2.0]\nwv, weights = gausslegendre(300)\nwv = (wv .+ 1) ./ 2 * (w_interval[2] - w_interval[1]) .+ w_interval[1]\nc = (w_interval[2] - w_interval[1]) / 2\n\nks(t) = c * sum(S.(wv) .* cos.(2 * π * wv * t) .* weights);","category":"page"},{"location":"examples/1-densities/#Results","page":"Kernel Densities","title":"Results","text":"","category":"section"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"We see that we indeed recover the kernel from the spectral density, with only a small error from the numerical integration.","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"f = Figure(; size=(600, 400))\nax = Axis(f[1, 1]; xlabel=\"τ\", ylabel=\"k(τ)\")\nlines!(ax, τv, k.(τv); label=\"kernel\")\nlines!(ax, τv, ks.(τv); label=\"spectral approx\")\naxislegend()\nf","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"(Image: )","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"<hr />\n<h6>Package and system information</h6>\n<details>\n<summary>Package information (click to expand)</summary>\n<pre>\nStatus &#96;~/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl/examples/1-densities/Project.toml&#96;\n  &#91;13f3f980&#93; CairoMakie v0.13.1\n  &#91;0b91fe84&#93; DisplayAs v0.1.6\n  &#91;31c24e10&#93; Distributions v0.25.117\n  &#91;442a2c76&#93; FastGaussQuadrature v1.0.2\n  &#91;027d52a2&#93; KernelSpectralDensities v0.2.0 &#96;/home/runner/work/KernelSpectralDensities.jl/KernelSpectralDensities.jl#main&#96;\n  &#91;98b081ad&#93; Literate v2.20.1\n  &#91;bac558e1&#93; OrderedCollections v1.8.0\n  &#91;37e2e46d&#93; LinearAlgebra v1.11.0\n</pre>\nTo reproduce this notebook's package environment, you can\n<a href=\"./Manifest.toml\">\ndownload the full Manifest.toml</a>.\n</details>\n<details>\n<summary>System information (click to expand)</summary>\n<pre>\nJulia Version 1.11.3\nCommit d63adeda50d &#40;2025-01-21 19:42 UTC&#41;\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux &#40;x86_64-linux-gnu&#41;\n  CPU: 4 × AMD EPYC 7763 64-Core Processor\n  WORD_SIZE: 64\n  LLVM: libLLVM-16.0.6 &#40;ORCJIT, znver3&#41;\nThreads: 1 default, 0 interactive, 1 GC &#40;on 4 virtual cores&#41;\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE &#61; eager\n  JULIA_LOAD_PATH &#61; :/home/runner/.julia/packages/JuliaGPsDocs/7M86H/src\n</pre>\n</details>","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"","category":"page"},{"location":"examples/1-densities/","page":"Kernel Densities","title":"Kernel Densities","text":"This page was generated using Literate.jl.","category":"page"}]
}
